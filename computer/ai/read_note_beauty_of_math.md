# 《数学之美》阅读笔记

## 第一版序言（李星）

1. 如何阅读本书的典故
    * 典故里最核心的是相关历史事件中的人物
    * 我们必须要问：提出巧妙数学思想的人是谁？为什么是“他/她”提出了这个思想？其思维方法有何特点？
    * 成为一个领域的大师有其偶然性，但更有其必然性，其必然性就是大师们的思维方法

2. 从事科学研究最重要的是掌握思维方法
    * 简单性原则
        * 牛顿的《自然哲学的数学原理》：“法则1：除那些真实而已足够说明其现象者外，不必去寻找自然界事物的其他原因。”
        * 爱因斯坦：“从希腊哲学到现代物理学的整个科学史中，不断有人力图把表面上极为复杂的自然现象归结为几个简单的基本概念和关系。这就是整个自然哲学的基本原理。”
    * WWW的发明人蒂姆·伯纳斯·李谈设计原理：“简单性和模块化是软件工程的基石；分布式和容错性是互联网的生命。”

3. 有“正确设计思想方法的技术”未必能够成功，因为还有非技术的因素；但“没有正确设计思想方法的技术”一定失败，无一例外。

## 第二版前言

1. 数学的作用：“且不说那些和我们生活联系相对较少的领域，比如原子能和航天，都需要用到大量的数学知识。就说我们天天用的产品和技术，背后都有支持它们的数学基础。作为一名工作了20多年的科学工作者，我在工作中经常惊叹数学语言应用于解决实际问题时的魔力。”

2. 数学之美
    * 简单性：数学之美，首先在于其内容或许复杂而深奥，但形式常常很简单。
    * 通用性和普遍性：数学之美还在于数学原理的通用性和普遍性。一个好的数学模型，常常能解决一系列，甚至是许多看似毫不相干领域的实际问题。

## 第一章 文字和语言 vs 数字和信息

1. 数字、文字、语言的联系
    * 都是信息的载体
    * 共同目的：记录和传播信息

2. 文字的起源
    * 早期人类利用声音来传播信息
    * 早期人类迅速学习新鲜事物，语言越来越丰富
    * 语言描述的共同要素（比如物体、数量和动作）抽象处理，形成词汇
    * 词汇多到人类仅靠大脑记不住的时候，便产生高效记录信息的需求，也就是文字

3. 象形文字
    * 古埃及人发明了最早的保存信息的方式：用图形表示事物（象形文字）
    * 埃及象形文字增加到5000个左右便不再增加，因为没有人能够学会和记住这么多文字，于是出现一词多义的现象
    * 多义字需要依靠上下文去除歧义性

4. 翻译之所以可行，仅仅是因为不同的文字系统在记录信息上的能力是等价的。

5. 罗塞塔石碑的启示
    * 信息的冗余是信息安全的保障
    * 语料（语言的数据），尤其是双语或多语的对照语料对翻译至关重要，它是我们从事机器翻译研究的基础

6. 数字的起源
    * 数字出现在人们的财产多到需要数一数才搞清楚有多少的时候
    * 当我们的祖先发现十个指头不够用了，就发明了进位制，逢十进一
    * 玛雅文明采用二十进制

7. 拼音文字的起源
    * 楔形文字在两河流域的美索不达米亚地区诞生
    * 楔形文字是我们这个星球上最古老的拼音文字
    * 腓尼基人不愿意花大量时间雕刻楔形字母，而将它们简化为22个字母
    * 拼音文字在古希腊得到大力发展，古希腊文字母的拼写和语音紧密结合在一起

8. 自然语言背后的科学规律
    * 从象形文字到拼音文字，体现了从物体的外表进化到了抽象的概念，同时不自觉地采用了对信息的编码
    * 罗马体系的文字中，常用字短，生僻字长，符合信息论中的最短编码原理
    * 在古代，两个人讲话说得快是一个宽信道，无需压缩；书写来得慢是一个窄信道，需要压缩，符合信息论原理——信道宽不压缩，信道窄要尽可能压缩
    * 将日常的白话口语写成精简的文言文是信道压缩的过程，而将文言文解释清楚是解压缩的过程
    * 犹太人抄写《圣经》时使用了类似校验码的方法来发现抄写错误

9. 词可以被认为是有限而封闭的集合，而语言则是无限和开放的集合。因此，任何语言都有语法规则覆盖不到的地方。

10. 语言学研究方法
    * 问题：到底是语言对，还是语法对
    * 前者坚持从真是的语句文本（即语料）出发，而后者坚持从规则出发。

## 第2章 自然语言处理：从规则到统计

1. 语言的本质：语言交流的过程就是编码和解码的过程

2. 自然语言处理的两个阶段
    * 20世纪50年代～70年代：走弯路。用电脑模拟人脑，成果近乎为零
    * 20世纪70年代后：基于数学模型和统计的方法，取得实质性的突破

3. 早期认识
    * 鸟飞派：看看鸟是怎样飞的，就能模仿鸟造出飞机，而不需了解空气动力学；要让机器完成翻译或者语音识别等事情，就必须先让计算机理解自然语言
    * 要想理解自然语言，首先要做好两件事：分析语句和获取语义

4. 重写规则：分析句子采用的文法规则

5. 20世纪60年代，基于乔姆斯基形式语言的编译器技术得到很大发展，计算机高级程序语言都可以概括成上下文无关的文法

6. 基于文法分析来理解自然语言的困难
    * 文法规则的数量太大
    * 自然语言的文法是上下文有关文法，很难用计算机来解析

7. 基于语义处理来理解自然语言的困难
    * 自然语言中词的多义性很难用规则来描述，而是严重依赖于上下文
    * 1966年明斯基举反例说明计算机处理语言的难处，导致自然语言处理研究停滞

8. 1970年以后统计语言学的出现使得自然语言处理重获新生

9. 基于规则和基于统计的争执
    * 1988年提出的基于统计的机器翻译方法，框架是对的，但效果很差，因为当时既没有足够的统计数据，也没有足够强大的模型来解决不同语言语序颠倒的问题
    * 基于规则的捍卫者攻击对方的武器：基于统计的方法只能处理浅层的自然语言处理问题，而无法进入深层次的研究
    * 用基于统计的方法代替传统的方法，需要等原有的一批语言学家退休

10. 基于统计的自然语言处理方法，在数学模型上和通信是相通的，甚至就是相同的

## 第3章 统计语言模型

1. 统计语言模型：为自然语言建立的数学模型，它是今天所有自然语言处理的基础。

2. 统计语言模型产生的初衷是为了解决语音识别问题。

3. 贾里尼克针对语音识别提出的统计模型：一个句子是否合理，就看它的可能性如何，或者说它出现的概率如何。

4. 马尔可夫假设：文本中任意一个词出现的概率只同它前面一个词有关。

5. N-1阶马尔可夫假设：文本中每个词只和前面N-1个词有关。

6. 古德-图灵估计：对于没有看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量中，分配一个很小的比例给这些没有看见的事件。

7. 模型训练中的一个重要问题是训练数据或者说语料库的选择。如果训练语料和模型应用的领域相脱节，那么模型的效果往往会大打折扣。

## 第4章 谈谈分词

1. 1990年左右，当时在清华大学电子工程系工作的郭进博士用统计语言模型成功解决了分词二义性问题，将汉语分词的错误率降低了一个数量级。

2. 可以把找出概率最大的分词方法看成一个动态规划问题，并利用维特比算法快速地找到最佳分词。

3. 统计语言模型很大程度上是依照”大众的想法“，或者”多数句子的用法“，而在特定情况下可能是错的。

4. 原来用于对中文进行分词的技术，也在英语的手写体识别中派上了用场。

## 第5章 隐含马尔可夫模型

1. 所谓语音识别，就是听者去猜测说话者要表达的意思。这其实就像通信中，接收端根据收到的信号去分析、理解、还原发送端传送过来的信息。

2. 隐含马尔可夫模型：任一时刻的状态是不可见的，所以观察者无法通过观察到一个状态序列来推测转移概率等参数。但隐含马尔可夫模型在每个时刻会输出一个符号，而且此符号只与当前的状态相关。

## 第6章 信息的度量和作用

1. 信息量等于不确定性的多少，可以用”信息熵“来度量。

2. 信息的作用：信息是消除系统不确定性的唯一方法。在没有获得任何信息前，一个系统就像是一个黑盒子，引入信息，就可以了解黑盒子系统的内部结构。

3. 网页搜索本质上是利用信息消除不确定性的过程。因为网页搜索要从大量（几十亿个）网页中，找到和用户输入的搜索词最相关的几个网页。

## 第7章 贾里尼克和现代语言处理

1. 吴军：一个人想要在自己的领域做到一流，他的周围必须有非常多的一流人物。

2. 贾里尼克：我每开除一名语言学家，我的语音识别系统识别率就会提高一点。

3. 贾里尼克把语音识别问题当成通信问题，并用两个隐含马尔可夫模型（声学模型和语言模型）把语音识别概括得清清楚楚。

## 第8章 简单之美：布尔代数和搜索引擎

1. 技术分为术和道两种，具体的做事方法是术，做事的原理和原则是道。

2. 真正做好一件事没有捷径，离不开一万小时的专业训练和努力。做好搜索，最基本的要求是每天分析10~20个不好的搜索结果，累积一段时间才会有感觉。Google的搜索质量第一技术负责人阿米特·辛格至今依然经常分析那些不好的搜索结果。

3. 建立一个搜索引擎的步骤：
    * 下载：自动下载尽可能多的网页
    * 索引：建立快速有效的索引
    * 排序：根据相关性对网页进行公平准确的排序

4. 文献检索与布尔运算的关系：搜索引擎根据每篇文献是否包含某个关键词，给每篇问下一个逻辑值（真/假）。

5. 搜索引擎快速找到搜索结果的方法：建索引。

6. 由于索引数量巨大，需要通过分布式的方式来存储和处理索引。

7. 牛顿：人们发觉真理在形式上从来是简单的，而不是复杂和含混的。（Truth is ever to be found in simlicity, and not in the multiplicity and confusion of thing.）

## 第９章　图论和网络爬虫

1. 哥尼斯堡的七座桥
    * 问题：能否将哥尼斯堡的七座桥恰好走过一遍并回到原出发点？
    * 求解：欧拉通过以下定理证明了这种走法是不可能的。
    * 定理：如果一个图能够从一个顶点出发，每条边不重复地遍历一遍回到这个顶点，那么每一顶点的度必须为偶数。
    * 度：某个顶点的度是指与之相连的边的数量。

2. 网络爬虫
    * 定义：从一个网页出发，用图的遍历算法，自动地访问到每一个网页并把它们存起来。完成这个功能的程序叫做网络爬虫。
    * 起源：世界上第一个网络爬虫是由麻省理工学院的学生马休·格雷在１９９３年写成的，他将其命名为“互联网漫游者”。
    * 网页遍历次序：网络爬虫对网页遍历的次序不是简单的ＢＦＳ或者ＤＦＳ，而是有一个相对复杂的下载优先级排序的方法。

## 第10章 PageRank：Google的民主表决式网页排名技术

1. 搜索结果的排名取决于两组信息
    * 关于网页的质量信息
    * 这个查询与每个网页的相关性信息

2. PageRank
    * 核心思想：如果一个网页被很多其他网页所链接，说明它受到普遍的承认和信赖，那么它的排名就高。
    * 考虑权重：网页排名高的网站贡献的链接权重大。

3. 网页排名的计算
    * 衡量方法：一个网页的排名来自于所有指向这个网页的其他网页的权重之和。
    * 转化为二维矩阵相乘的问题
    * 使用迭代方法来求解
    * 运用稀疏矩阵计算的技巧简化计算量
    * 通过MapReduce并行计算工具大大缩短计算时间

3. PageRank的来源
佩奇：“当时我们觉得整个互联网就像一张大的图，每个网站就像一个节点，而每个网页的链接就像一个弧。我想，互联网可以用一个图或者矩阵描述，我也许可以用这个发现做篇博士论文。”

## 第11章 如何确定网页和查询的相关性

1. 影响搜索引擎质量的四大因素
    * 完备的索引
    * 对网页质量的度量
    * 用户偏好
    * 确定一个网页和某个查询的相关性的方法

2. TF-IDF（Term Frequency / Inverse Document Frequency）
    * 作用：搜索关键词权重的科学度量
    * IDF：Inverse Document Frequency，逆文本频率指数。一个关键词在越多的网页中出现过，其IDF指数就越小。
    * 相关性计算：关键字词频的加权求和，权重即IDF的值

## 第12章 有限状态机和动态规划：地图和本地搜索的核心技术

1. 本地生活服务：确认地点、查看地图、查找路线等等。

2. 有限状态机的结构
    * 一个特殊的有向图，包括一些状态（节点）和连接这些状态的有向弧
    * 有一个开始状态、一个终止状态和若干中间状态，每条弧上带有一个状态进入下一个状态的条件
    * 应用：有限状态机和动态规划的应用非常广泛，包括地图服务、语音识别、拼写和语法纠错、拼音输入法、工业控制和生物的序列分析等。

3. 地址分析
    * 地址的文法是上下文有关文法中相关简单的一种，可以使用有限状态机来分析
    * 分析方法：如果一条地址能从状态机的开始状态经过状态机的若干中间状态，走到终止状态，则这条地址有效，否则无效。
    * 模糊匹配：当用户输入的地址不太标准或有错别字时，需要模糊匹配，并给出一个字串为正确地址的可能性，为此提出了基于概率的有限状态机。

## 第13章 Google AK-47的设计者：阿米特·辛格博士

1. AK-47冲锋枪：从不卡壳，不易损坏，可在任何环境下使用，可靠性好，杀伤力大并且操作简单。

2. 吴军：在计算机科学领域，一个好的算法应该像AK-47冲锋枪那样：简单、有效、可靠性好而且容易读懂（或者说易操作），而不应该是故弄玄虚。

3. 辛格做事情的哲学：先帮助用户解决80%的问题，再慢慢解决剩下的20%问题。这是在工业界成功的秘诀之一。许多失败并不是因为人不优秀，而是做事情的方法不对，一开始追求大而全的解决方案，之后长时间不能完成，最后不了了之。

4. 辛格坚持选择简单方案的另一个原因是容易解释每一个步骤和方法背后的道理，这样不仅便于出问题时差错，而且容易找到今后改进的目标。

5. 辛格之所以总是能找到那些简单有效的方法，不是靠直觉或运气，这首先靠他丰富的研究经验。其次，辛格坚持每天要分析一些搜索结果不好的例子，以掌握第一手的资料，即使在他成为Google Fellow以后，依旧如此。

## 第14章 余弦定理和新闻的分类

1. 新闻的分类很大程度上依靠的是余弦定理。

2. 新闻的数学表示
    * 为了让计算机能够“算”新闻（而不是读新闻），需要先把文字的新闻变成一组可计算的数字，再设计一个算法来算出任意两篇新闻的相似性。
    * 新闻的特征向量：对于一篇新闻中的所有实词，计算出它们的TFIDF值，把这些值按照对应的实词在词汇表的位置依次排列，就得到一个向量。

3. 新闻相似度的衡量：通过计算两个向量的夹角来判断对应的新闻主题的接近程度。而要计算两个向量的夹角，就要用到余弦定理。

4. 新闻分类的算法
    * 已知各类新闻的特征向量：计算待分类新闻与各类新闻特征向量的余弦相似性，并将其归入到距离最近的那一类中。
    * 没有各类新闻的特征向量：使用自底向上不断合并的办法，即不断把相似性大于一个阀值的新闻合并成一个小类。

5. 美国人做事的一个习惯：美国人总是倾向于用机器（计算机）代替人工来完成任务。虽然在短期内需要做一些额外的工作，但从长远来看可以节省很多时间和成本。

## 第15章 矩阵运算和文本处理中的两个分类问题

1. 自然语言处理中最常见的两个分类问题
    * 将文本按主题归类，比如将所有介绍奥运会的新闻归到体育类
    * 将词汇表中的字词按意思归类，比如将各种运动的项目名称都归到体育类

2. 这两个分类问题都可以通过矩阵运算来圆满地、一次性地解决。

3. 一次性将所有新闻相关性计算出来：利用矩阵运算中的奇异值分解（Singular Value Decomposition, SVD）。

4. 奇异值分解，就是把一个大矩阵分解成三个小矩阵相乘。

5. 奇异值分解的优缺点：
    * 优点：能较快地得到结果，不需要一次次地迭代
    * 缺点：得到的分类结果略显粗糙，因此适合处理超大规模文本的粗分类

6. 奇异值分解与余弦定理结合：在实际应用中，可以先进行奇异值分解，得到粗分类结果；在此基础上，利用计算向量余弦的方法，进行几次迭代，得到比较精确的结果。

## 第16章 信息指纹及其应用

1. 信息指纹
    * 定义：任何一段信息（包括文字、语音、视频、图片等），都可以对应一个不太长的随机数，作为区别这段信息和其他信息的指纹。只要算法设计得好，任意两段信息的指纹都很难重复，就如同人类的指纹一样。
    * 应用场景：信息指纹在加密、信息压缩和处理中都有着广泛的应用。

2. 信息指纹的计算方法一般分为两步
    * 将字符串看成是一个特殊的、很长的整数
    * 使用伪随机数产生器算法（Pseudo-Random Number Generator, PRNG），将任意很长的整数转换成特定长度的伪随机数

3. PRNG算法
    * 最早的PRNG算法是由冯·诺依曼提出的，其方法很简单，就是将一个数的平方掐头去尾，取中间的几位数。
    * 现在常用的PRNG算法是梅森旋转算法。
    * 在互联网上加密要使用基于加密的伪随机数产生器（Cryptographically Secure Pseudo-Random Number Generator, CSPRNG），常用的算法有MD5或者SHA-1等标准，它们可以将不定长的信息变成定长的128或者160位二进制随机数。

4. 信息指纹的用途
    * 判定集合相同：计算这两个集合的指纹，然后直接进行比较。可以用来判断一个人是否用两个不同的Email账号对同一组人群发垃圾邮件。
    * 判定集合基本相同：随机挑选几个元素计算其指纹，然后进行比较。可以用来判断两个网页是否是重复的。
    * 判断一篇文章是否抄袭了另一篇文章：将每一篇文章切成小的片段，挑选出这些片段的特征词集合，计算并比较这些指纹，就能找出大段相同的文字，最后根据时间先后，找出原创的和抄袭的。
    * 判断一个视频是否为另一个视频的盗版：找到关键帧，用信息指纹表示这些关键帧，最后利用判断集合是否相同的方法来检查是否盗版。

## 第17章 由电视剧《暗算》所想到的：谈谈密码学的数学原理

1. 最早的密码：密码学的历史大致可以追溯到两千年前，相传古罗马名将凯撒为了防止敌方截获情报，用密码传送情报。凯撒的做法很简单，就是对二十几个罗马字母建立一张对应表。

2. 对于一种好的编码方法，破译者应该无法从密码中统计出明码的规律。

3. 好的密码必须做到根据已知的明文和密文推断不出新的密文内容。

4. 密码的最高境界是敌方在截获密码后，对我方的所知没有任何增加，用信息论的专业术语讲，就是信息量没有增加。

5. 公开密钥的原理
    * 有两个完全不同的样式，一个用于加密，一个用于解密。
    * 这两个看上去无关的钥匙，在数学上是关联的。

6. 我们今天用的所谓最可靠的加密方法，背后的数学原理其实就这么简单，一点也不神秘。无论是RSA算法、Rabin算法还是后来的EI Gamal算法，无非是找几个大素数做一些乘除和乘方运算。

## 第18章 闪光的不一定是金子：谈谈搜索引擎反作弊问题和搜索结果的权威性问题

1. 通信中解决噪音干扰问题的基本思路
    * 从信息源出发，加强通信（编码）自身的抗干扰能力
    * 从传输来看，过滤掉噪音，还原信息

2. 搜索引擎作弊从本质上看就如同对搜索排序的信息加入噪音，因此反作弊的第一条是要增强排序算法的抗噪声能力，其次是像在信号处理中去噪音那样，还原原来真实的排名。

3. 计算权威度时，我们需要用到句法分析、互信息和短语的聚类，而它们的背后都是数学。因此可以说，对搜索结果权威性的度量，完全是建立在各种数学模型基础之上的。

## 第19章 谈谈数学模型的重要性

1. 托勒密发明了球坐标，定义了包括赤道和零度经线在内的经纬线，提出了黄道，还发明了弧度制。他最大最有争议的贡献是对地心说模型的完善。

2. 在当时，从人们的观测出发，很容易得到地球是宇宙中心的结论。而从地球上看，行星的运动轨迹是不规则的，托勒密的伟大之处是用40~60个在大圆上套小圆的方法，精确地计算出了所有行星运动的轨迹。托勒密模型的精度之高，让后来所有的科学家都惊叹不已。即使今天在计算机的帮助下，我们也很难解出40个套在一起的圆的方程。

3. 在所有一流的天文学家中，开普勒资质较差，一生中犯了无数低级的错误。但是他有两样别人没有的东西，首先是从他的老师弟谷手中继承的大量的、在当时最精确的观测数据，其次是运气。

4. 吴军在腾讯内部技术讲座结束时总结过以下几点结论：
    * 一个正确的数学模型应当在形式上是简单的。（托勒密的模型显然太复杂）
    * 一个正确的模型一开始可能还不如一个精雕细琢过的错误模型来的准确，但是如果我们认为大方向是对的，就应该坚持下去。（日心说一开始并没有地心说准确）
    * 大量准确的数据对研发很重要。
    * 正确的模型也可能受噪音干扰，而显得不准确；这时不应该用一种凑合的修正方法加以弥补，而是要找到噪音的根源，这也许能通往重大的发现。

## 第20章 不要把鸡蛋放在一个篮子里：谈谈最大熵模型

1. 最大熵原理：保留全部的不确定性，将风险降得最小。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。

2. 最大熵原理指出，对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们把这种模型叫做“最大熵模型”。

3. 拉纳帕提：第一个在实际信息处理应用中验证了最大熵模型的优势
    * 拉纳帕提的聪明之处在于他没有对最大熵模型进行近似处理，而是找到了几个最适合用最大熵模型而计算量相对不太大的自然语言处理问题，比如词性标注和句法分析。
    * 拉纳帕提成功地将上下文信息、词性以及主谓宾等句子成分，通过最大熵模型结合起来，做出了当时世界上最好的词性标识系统和句法分析器。

4. 最大熵模型，可以说是集简繁于一体：形式简单，实现复杂。

## 第21章 拼音输入法的数学原理

1. 过去的25年里，中文输入法基本上经历了以自然音节编码输入，到偏旁笔画拆字输入，再回归自然音节输入的过程。

2. 对汉字的编码分为两部分：对拼音的编码和消除歧义性的编码。

3. 虽然拼音输入法看上去输入每个汉字需要多敲几个字，但是有三个优点让它的输入速度并不慢。
    * 它不需要专门学习
    * 输入自然，不会中断思维。也就是说找每个键的时间非常短
    * 因为编码长，有信息冗余量，容错性好

4. 拼音转汉字的算法和在导航中寻找最短路径的算法相同，都是动态规划。数学的妙处在于它的每一个工具都具有相当的普遍性，在不同的应用中都可以发挥很大的作用。

## 第22章 自然语言处理的教父马库斯和他的优秀弟子们

1. 将自然语言处理从基于规则的研究方法转到基于统计的研究方法，贡献最大的两个人是：贾里尼克、米奇·马库斯。

2. 马库斯对这个领域的贡献体现在他造福于全世界研究者的宾夕法尼亚大学LDC语料库以及他的众多优秀弟子。

3. 马库斯利用自己的影响力，推动美国自然科学基金会（National Science Foundation，NSF）和DARPA出资立项，联络了多所大学和研究机构，建立了数百个标准的语料库组织（Linguistic Data Consortium，LDC）。

4. 过去20年里，在机器学习和自然语言处理领域，80%的成果来自于数据量的增加。马库斯对这些领域数据的贡献可以说是独一无二的。

5. 由于马库斯宽松的管理方式，他培养的博士生在研究和生活中都是个性迥异。有些人善于找到间接快速的方法和容易做出成绩的题目，有的人习惯啃硬骨头；有些人三四年就拿到博士去当教授了，而有些人“赖在”学校里七八年不走，最后出一篇高质量的博士论文。这些各有特点的年轻学者，后来分别能适应文化迥异的各个大学和公司。

6. 柯林斯：追求完美
    * 在读博士期间，柯林斯写了一个后来以他的名字命名的自然语言文法分析器，他做文法分析器的出发点不是为了验证一个理论，而是要做一个世界上最好的分析器。
    * 柯林斯成功的关键在于将文法分析的每一个细节都研究得很仔细，他用的数学模型也很漂亮，整个工作可以用完美来形容。
    * 柯林斯的博士论文堪称自然语言处理领域的范文。它像一本优秀的小说，把所有事情的来龙去脉介绍得清清楚楚。
    * 柯林斯的特点就是把事情做到极致。如果说有人喜欢“繁琐哲学”，柯林斯就是一个。

7. 布莱尔：简单才美
    * 与柯林斯的研究方法相反，布莱尔总是试图寻找简单得不能再简单的方法。
    * 布莱尔的成名作是基于变换规则的机器学习方法。名字看似很复杂，其实非常简单。
    * 如果说柯林斯是个“务于精纯”的精深专才，布莱尔则更像“观其大略”的通才。

## 第23章 布隆过滤器

1. 在日常生活或工作中经常要判断一个元素是否在一个集合中
    * 在字处理软件中，需要检查一个英语单词是否拼写正确（也就是判断它是否在已知的字典中）
    * 在FBI中，需要核实一个嫌疑人的名字是否已经在嫌疑名单上
    * 在网络爬虫中，需要确认一个网址是否已访问过

2. 判断元素是否在集合中的最直接方法：散列表
    * 将集合中全部元素以散列表的方式存储在电脑中，遇到新元素时直接查散列表
    * 优点：快速准确
    * 缺点：耗费存储空间

3. 1970年伯顿·布隆提出布隆过滤器
    * 只需要散列表1/8到1/4的大小就能解决同样的问题
    * 布隆过滤器实际上是一个很长的二进制向量和一系列随机映射函数
    * 布隆过滤器背后的数学原理在于两个完全随机的数字相冲突的概率很小，因此可以在很小的误识别率条件下，用很少的空间存储大量信息
    * 补救误识别的常见办法是再建立一个小的白名单，存储那些可能被误判别的信息
    * 布隆过滤器中只有简单的复杂运算，因此速度很快，使用方便

4. 针对电子邮件地址的布隆过滤器的建立
    * 假设存储一亿个电子邮件地址，先建立一个16亿个比特位的向量，并将其全部清零
    * 对于每个电子邮件地址，用8个不同的随机数产生器产生8个信息指纹
    * 再用一个随机数产生器把这8个信息指纹映射到1~16亿中的8个自然数
    * 最后把8个自然数对应的8个位置的比特位全部设置为1
    * 对这一亿个电子邮件地址都进行这样的处理后，一个针对电子邮件地址的布隆过滤器就建成了

## 第24章 马尔可夫链的扩展：贝叶斯网络

1. 马尔可夫链模型的局限性：在现实生活中，很多事物相互的关系并不能用一条链来串起来，很可能是交叉的、错综复杂的。

2. 从数学的层面来讲，贝叶斯网络是一个加权的有向图，是马尔可夫链的扩展（在贝叶斯网络中马尔可夫假设仍然成立）；而从认识论的层面看，贝叶斯网络克服了马尔可夫链那种机械的线性的约束，它可以把任何有关联的事件统一到它的框架下面。

3. 贝叶斯网络的应用：贝叶斯网络在文本分类、概念抽取、生物统计、图像处理、决策支持系统和博弈论中都有广泛应用。

## 第25章 条件随机场、文法分析及其他

1. 计算语言学家尤金·查尼阿克统计出文法规则的概率，在选择文法规则时，坚持一个原则：让被分析的句子的语法树概率达到最大。

2. 拉纳帕提从全新的角度来看待文法分析问题：把文法分析看成是一个括括号的过程。

3. 条件随机场是一种特殊的概率图模型，也是隐含马尔可夫模型的一种扩展。它和贝叶斯网络的区别在于：条件随机场是无向图，而贝叶斯网络是有向图。

4. 条件随机场在自然语言处理、模式识别、机器学习、生物统计，甚至预防犯罪等方面都有很成功的应用。

## 第26章 维特比和他的维特比算法

1. 维特比算法是一个特殊但应用最广的动态规划算法，它是针对一个特殊的图——篱笆网络的有向图最短路径问题而提出的。

2. 维特比算法之所以重要，是因为凡是使用隐含马尔可夫模型描述的问题都可以用它来编码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。

3. 维特比为推广自己的算法做了两件事：
    * 首先，他放弃了这个算法的专利
    * 第二，它和雅各布博士一起在1968年创办了Linkabit公司，将这个算法做成芯片，卖给其他通信公司。

4. 维特比是少数能够将自己的研究成果应用到实际中的科学家之一。他不仅提供了关键性的发明，而且为了保障其效益在全社会得到最大化，他解决了所有配套的技术。

## 第27章 上帝的算法：期望最大化算法

1. 期望最大化算法（Expectation Maximization Algorithm, EM）
    * 首先，根据现有的模型，计算各个观测数据输入到模型中的计算结果，这个过程称为期望值计算过程（Expectation），或E过程
    * 然后重新计算模型参数，以最大化期望值，这个过程成为最大化的过程（Maximization），或M过程

2. EM算法只需要有一些训练数据，定义一个最大化函数，剩下的事情就交给计算机了。经过若干次迭代，我们需要的模型就训练好了。这实在是太美妙了，这也许是造物主刻意安排的。所以我把它称作上帝的算法。

## 第28章 逻辑回归和搜索广告

1. 早期百度广告系统与Google的区别
    * 百度是按广告主出价高低来排名的竞价排名广告。谁给钱多，就优先展示谁的广告
    * Google是综合出价和点击率（Click Through Rate，CTR）等因素决定广告的投放

2. 工业界普遍采用了逻辑回归模型来描述影响点击率的因素。

3. 逻辑回归模型是指将一个事件出现的概率逐渐适应到一条逻辑曲线上，逻辑曲线是一条S型曲线。

4. 逻辑回归模型是一种将影响概率的不同因素结合在一起的指数模型。和很多指数模型的训练方法相似，都可以采用通用迭代算法GIS和改进的迭代算法IIS来实现。

## 第29章 各个击破算法和Google云计算的基础

1. 云计算的关键之一是，如何把一个非常大的计算问题，自动分解到许多计算能力不是很强大的计算机上，共同完成。针对这个问题，Google给出的解决工具是一个叫MapReduce的程序，其根本原理就是十分常见的分治算法。

2. MapReduce的根本原理
    * 将一个大任务拆分成小的子任务，并且完成子任务的计算，这个过程叫做Map
    * 将中间结果合并成最终结果，这个过程叫做Reduce

## 第30章 Google大脑和人工神经网络

1. 人工神经网络是一种特殊的有向图，其节点叫做神经元，其有向弧则被看成是连接神经元的神经。其特殊性概括如下：
    * 图中所有节点都是分层的，每一层节点可以通过有向弧指向上一层节点，但是同一层节点之间没有弧互相连接，而且每一个节点不能越过一层连接到上上层的节点上。
    * 每一条弧上有一个值（称为权重或者权值），根据这些值，可以用一个非常简单的公式算出它们所指节点的值。

2. 人工神经网络的基本原理
    * 输入层的节点接受输入信息，并按照它们输出的弧的权重进行线性加权，然后再做一次函数变换，赋给第二层的节点
    * 第二层的节点照此数值向后面传递，直到第三层，如此一层层传递，直到最后一层，最后一层又被称为输出层

3. 人工神经网络中需要设计的部分只有两个
    * 它的结构，即网络分几层、每层几个节点、节点之间如何连接等等
    * 非线性函数的设计，常用的函数是指数函数

4. 人工神经网络的有向弧上的权重，即模型参数，是通过训练（学习）得到的。


5. Google大脑的训练算法与MapReduce的设计思想有很多相似之处，两者都使用了分治算法，不同的是Google大脑的分制算法更复杂。

6. Google大脑在减少计算量方面做了两个改进：
    * 采用随机梯度下降法，相比梯度下降法大大降低计算量，当然会牺牲一点准确性
    * 采用比一般梯度下降法收敛更快的L-BFGS(Limited-memory Broyden Fletcher Goldfarb Shanno Method)方法，减少训练的迭代次数

7. “Google大脑”说穿了是一种大规模并行处理的人工神经网络。它并不是一个什么都能思考的大脑，而是一个很能计算的人工神经网络。因此，与其说Google大脑很聪明，不如说它很能算。

## 第31章 大数据的威力：谈谈数据的重要性

1. 随着互联网的发展，特别是云计算的兴起和逐渐普及，计算机获取、存储和处理数据的能力快速提升，人们逐渐从大量的数据中发现了很多原本难以找到的归类，于是很多科学研究和工程领域都取得了以前难以想象的进步。

2. 数据的重要性
    * 人类的文明与进步，从某种意义上讲是通过对数据进行收集、处理和总结而达成的。
    * 近代自然科学领域里，科学家们很重要的一项工作就是做实验，而做实验的目的就是采集数据，因为科学发明需要通过这些数据来推导或证实。
    * 日常的很多感觉与数据给出的结论是相反的，如果不用数据说话，我们成功的几率就会小很多。
    * 数据不仅在科学研究中，而且在生活的方方面面都很重要，它应该成为我们日常做决策的依据。

3. 今天很多大学里的非数学专业将概率和统计放在一门课里教授，但其实概率论和统计学虽然紧密相关，却是独立发展的。概率论是研究随机现象数量规律的数学分支。统计学是通过搜索、整理、分析数据等手段，以达到推断所测对象的本质，深圳预测对象未来的一门综合性科学。

4. 统计要求数据量充足以及采样的数据具有代表性。

5. 大数据更重要的威力在于它的多维度和完备性，有了这两点才能将原本看似无关的时间联系起来，恢复出对事物全方位完整的描述。

6. 在未来的世界里，人们的生活会越来越离不开数据，很多围绕数据收集和处理的工作机会将不断涌现。而掌握处理和利用数据方法的人也必成为新时代的成功者。推而广之，无论在什么领域，从事什么样的工作，谁懂得数据的重要性，谁会在工作中善用数据，就更可能获得成功。

## 第二版后记

1. 吴军谈写《数学之美》的原因
    * 希望让做工程的年轻人看到在信息技术行业做事情的正确方法。无论是在美国还是中国，大部分软件工程师在一个未知领域都是从直觉感觉出发，用“凑”的方法来解决问题，说得不好听就是山寨。
    * 希望IT公司的工程主管能够带领部属，提高工程水平，逐渐远离山寨，这样才有可能真正接近世界一流IT公司的做事水准，避免大量低水平的重复建设导致的惊人浪费。
    * 介绍信息处理的规律性。任何事物都有它的发展规律，而这些规律都是可以认识的，在信息领域也不例外。香农创建的信息论在很大程度上指出了我们今天信息处理和通信领域的本质和规律。当我们认识了规律后，就应该自觉地在工作中遵循而非违背规律。
    * 通过对IT规律性的认识，读者可以举一反三地总结、学习、认识和自觉运用自己工作中的规律性，这样有助于将自己的境界提升一个层次。

2. 吴军谈对本书写作帮助最大的两本书和一个节目
    * 《从一到无穷大》：介绍宇宙的科普读物
    * 《时间简史》：把深奥的宇宙学原理用最简单的语言讲出来
    * 《穿越虫洞》：一个把当今最前沿的物理学做成了浅显易懂的节目，节目中有包括很多诺贝尔奖获得者在内的一流物理学家和数学家介绍他们的工作，他们都能用很简单的比喻将所在领域内最深奥的道理介绍清楚，让大众理解。
