# 《人工智能简史》读书笔记

## 前言

1. 作者赞赏的科普书
    * 安德鲁·霍奇斯的《艾伦·图灵传：如谜的解谜者》是内行写作的典范。
    * 安妮塔·费佛曼的两本逻辑学家传记是我心目中的标杆。

2. 写作目的
> 本书除了想梳理始于20世纪40年代的人工智能的历史外，还有一个作者隐含的心愿：作为人工智能的科普。

## 第1章 达特茅斯会议：人工智能的缘起

1. 人工智能的前戏
    * 1955年美国西部计算机联合大会中还套了一个小会：学习机讨论会。
    * 讨论会中塞弗里奇发表了一篇模式识别的文章，纽厄尔则探讨了计算机下棋，他们分别代表两派观点。
    * 神经网络的鼻祖之一皮茨的总结：“（一派人）企图模拟神经系统，而纽厄尔则企图模拟心智......但殊途同归。”
    * 这预示了人工智能随后几十年关于“结构与功能”两个阶级、两条路线的斗争。

2. 人物渊源
    * BASIC语言发明人（克门尼）曾是LISP语言发明人（麦卡锡）的老板。因为克门尼刚上任达特茅斯学院的系主任后，就从母校普林斯顿大学带回了包括麦卡锡在内的四位刚毕业的博士。
    * 麦卡锡的老师是失去双手的代数拓扑学家莱夫谢茨。
    * 明斯基的老师塔克是莱夫谢茨的学生。
    * 塞弗里奇在麻省理工学院时和麦卡洛克（神经网络的开创人之一）一起在维纳手下工作。
    * 香农当时是贝尔实验室的大佬。
    * 纽厄尔是麦卡锡和明斯基的同龄人，而司马贺比他们三人都大11岁。司马贺是纽厄尔的老师。

3. 纽厄尔和司马贺代表了人工智能的“符号派”，他们提出“物理符号系统假说”，简言之：智能是对符号的操作，最原始的符号对应于物理客体。

4. 麦卡锡和明斯基的建议书里罗列了他们计划研究的7个领域：
    * 自动计算机（“自动”指的是可编程）
    * 编程语言
    * 神经网络
    * 计算规模的理论，即计算复杂性
    * 自我改进，即机器学习
    * 抽象
    * 随机性和创见性

5. 会议中给所有人留下最深印象的是纽厄尔和司马贺的一款程序“逻辑理论家”
    * 这款程序可以证明怀特海和罗素《数学原理》中命题逻辑部分的一个很大子集。
    * 麦卡锡回忆说他从纽厄尔和司马贺的IPL语言中学到了表处理，这成为他后来发明LISP的基础。

6. 戴森在他的《一面多彩的镜子》一书中借鉴过柏林“刺猬与狐狸”的比喻
    * 刺猬是那些构建理论体系的人，狐狸则是那些解决问题的人
    * 在戴森眼里，爱因斯坦、哥德尔是刺猬，而费米、冯·诺依曼属于狐狸。

7. 乔姆斯基晚年和物理学家克劳斯对话时被问及“机器可以思维吗？”，他套用计算机科学家Dijkstra的说法反问：“潜艇会游泳吗？”

## 第2章 自动定理证明兴衰纪

1. 数学哲学有三大派：
    * 逻辑主义：代表人物是罗素，主旨是把数学归约到逻辑，这样只要把逻辑问题解决了，之上的数学问题自然就解决了。
    * 形式主义：代表人物是希尔伯特，他的梦想是把数学形式化，数学过程就是把一串符号变成另一串符号。他设想如果能设计一个大一统的算法，那么所有的数学问题都可以由这个算法来解答。哥德尔后来证明这一切是不可能的。
    * 直觉主义：代表人物是布劳威尔。

2. 自动定理证明起源于逻辑，初衷就是把逻辑演算自动化。逻辑学的源头是亚里士多德的三段论。

3. 戴维斯和普特南
    * 戴维斯于1954年完成了第一个定理证明程序，实现了普利斯博格算术的判定过程，可以证明“两个偶数之和还是偶数”之类的问题。
    * 戴维斯最重要的贡献是和哲学家普特南等人解决了希尔伯特第十问题（不定方程的可解答性）。
    * 普特南提出的“缸中脑”是最常被引用的假想实验之一。
    * 戴维斯和普特南在机器定理证明上合作的成果是影响广泛的戴维斯-普特南（Davis-Putnam，简称DP）过程，以及后来的DPLL。

4. 王浩
    * 1958～1959年实现了一个完全的命题逻辑程序和一个一阶逻辑程序，可以证明《数学原理》全部150条一阶逻辑以及200条命题逻辑定理。
    * 王浩研究了AE形式（即前面是全程量词，后面是存在量词）和AEA形式的可计算性和复杂性，由此引出了他的学生库克的NP理论。
    * 库克1971年发表的文章的题目是《定理证明的复杂性》，因此获得1982年图灵奖。
    * 王浩的定理证明研究孕育了整个理论计算机科学，他的定理证明程序后来也成为高级语言的基准程序。
    
5. 罗宾逊
    * 阿兰·罗宾逊拓展了普拉格维茨的原始合一算法，发明了归结原理。
    * 以前的定理证明技术会用到很多规则，现在所有证明推导只要有归结这一条规则就可以了。
    * 罗宾逊的贡献在于一系列工作的综合，除了归结外，还有合一和包含。

6. 马库恩
    * 用C语言写了Otter定理证明器，Otter实现了当时定理证明里最先进的所有技术。
    * 最早把项索引引入到机器定理证明器，并发明了差别树索引，极大地提高了证明的效率。
    * 利用Otter的模块开发了另一款专门证明方程的证明器EQP，并在1996年10月10日用EQP证明了罗宾斯猜想。

7. 哥德尔证明一阶整数（算术）是不可判定的，但几乎在同时塔尔斯基则证明一阶实数（初等几何和代数）是可判定的。

8. 数学家吴文俊在研究中国数学史时，受到塔尔斯基算法的启发，针对某一大类的初等几何问题给出了高效的算法，后来还推广到一类微分几何问题上。

9. 波尔和摩尔
    * 形式主义定理器中最广为人知的当属波尔-摩尔证明器及其一系列后续变种。波尔-摩尔证明器的核心是数学归纳法和项重写，可用来证明软件和硬件的正确性。
    * 波尔-摩尔字符串匹配算法（Boyer-Moore string search algorithm）是目前最快的字符串匹配算法，其起因也是定理证明。
    * 他们最早用来实现定理证明器的语言是InterLisp，但后来迅速发现InterLisp内带的朴素字符串搜索太慢，于是发明了自己的字符串匹配算法。

10. 定理证明的过程，都是一个归约的过程，无论是逻辑派的（即把数学问题归约到更基本的逻辑问题），还是形式派的（即用一套规则不断地变换给定的公式直到显性的形式出现）。

11. 定理证明的应用
    * 定理证明是极端的符号派。
    * 所有符号派的人工智能技术的基础都是定理证明，如专家系统、知识表示和知识库（甚至数据库）。
    * 专家系统的很多术语都是重新包装过的定理证明术语。如“知识库”就是“公理集合”，“规则库”就是“支持集”，“推理引擎”更是直接照搬。
    * 当下流行的知识图谱的基础也是定理证明技术——知识表示的理论“描述逻辑”就是被约束的一阶逻辑的子集。

12. 所有定理证明系统的一个致命问题是它们多是独立的，很少和其他数学工具结合，结果必然是只能是玩具系统，而不具实用性。

## 第3章 从专家系统到知识图谱

1. 费根鲍姆、李德伯格和翟若适合作发明了第一个专家系统DENDRAL。DENDRAL输入的是质谱仪的数据，输出是给定物质的化学结构。

2. 肖特莱福在布坎南的指导下发明了专家系统MYCIN，一个针对细菌感染的诊断系统。MYCIN的处方准确率为69%，当时专科医生的准确率为80%，MYCIN的成绩已经优于非本专业的医生。

3. MYCIN首创了后来作为专家系统要素的产生式规则：不精确推理。DENDRAL的初衷则是从专家采集来的数据做机器归纳，或者说机器学习。

4. 专家系统最成功的案例是DEC的专家配置系统XCON。当客户订购DEC的VAX系列计算机时，XCON可以按照需求自动配置零部件。从1980年投入使用到1986年，XCON一共处理了八万个订单。

5. 心理学家米勒和乔姆斯基等一起开拓了认知科学，晚年还带领普林斯顿大学的认知科学实验室同仁做了WordNet（“词网”）。WordNet不单是一个同义词辞典，还定义了词的上下位关系，WordNet成为自然语言处理的基本工具。

6. 雷纳特
    * 当雷纳特来到MCC（微电子与计算机技术公司）时，他已经有了一个新想法：把人类的常识编码，建成知识库。这个新项目叫Cyc。
    * Cyc取自英文单词encyclopedia（百科全书），这其实就是最早的知识图谱。
    * 雷纳特坚定地支持他老师费根鲍姆的知识原则：一个系统之所以能展示高级的智能理解和行为，主要是因为在所从事的领域所表现出来的特定知识：概念、事实、表示、方法、比喻以及启发。雷纳特甚至说：“智能就是一千万条规则。”
    * 雷纳特曾说：“学习只在已知事物的边缘发生，所以人们只可能学到与自己已知相似的新东西。如果你试图学习的东西与你已知的东西距离不远，那么你就能学会。这个边缘的范围越大（你已知的东西越多），就越有可能发现新的东西。”

7. 归根到底，专家系统的理论基础依然是机器定理证明。

8. 知识与推理
    * 如果从纯粹的定理证明的角度简单地看专家系统，所谓知识其实就是公理，公理越多，推理的步骤自然就会越少。
    * 所谓知识和推理的对立，其实是狭义（特殊目的）和广义（通用）的区别。知识是狭义的，推理是广义的，因为不需要过多的公理。
    * 狭义对机器的短期实现高效，但人的学习门槛较高；广义对机器的学习自然低效，但人学习的门槛较低。
    * 一阶逻辑的学习门槛最低，但当知识库变大，推理引擎也得变得更加专用才能高效。

## 第5章 神经网络简史

1. 1943年，麦卡洛克和皮茨发表了第一篇模拟神经网络的原创文章。

2. 罗森布拉特和感知机
    * 1957年，康奈尔大学的实验心理学家罗森布拉特模拟实现了一种叫做“感知机”的神经网络模型。
    * 罗森布拉特在理论上证明了单层神经网络在处理线性可分的模式识别问题时可以收敛，并以此为基础做了若干“感知机”有学习能力的实验。
    * 明斯基和佩珀特在《感知机：计算几何学》一书中证明单层神经网络不能解决XOR（异或）问题，进而证明神经网络的计算能力实在有限。
    * 明斯基呈现出的“感知机”的缺陷，对罗森布拉特是个致命打击，原来的政府资助机构也逐渐停止对神经网络研究的支持。

3. 1974年，哈佛大学的沃波斯的一篇博士论文证明了在神经网络多加一层，并且利用“后向传播”学习方法，可以解决XOR问题。但由于当时是神经网络研究的低估，文章不受重视。

4. 霍普菲尔德
    * 神经网络在20世纪80年代的复兴归功于物理学家霍普菲尔德。
    * 1982年，加州理工学院的生物物理教授霍普菲尔德提出一种新的神经网络——“霍普菲尔德网络”，可以解决一大类模式识别问题，还可以给出一类组合优化问题的近似解。

5. 连接主义运动
    * 一帮早期神经网络研究的幸存者，在生物学家克里克和认知科学大佬诺曼的鼓励下，以加州大学圣地亚哥分校为基地，开始了连接主义运动，这个运动的领导者是两位心理学家鲁美尔哈特和麦克利兰德，外加一位计算机科学家辛顿。
    * 连接主义运动的成果之一是那本被称为PDP（Parallel and Distributed Processing）的著名文集，这本书被后来的神经网络新秀称为“圣经”。

6. 机器翻译的早期实践都源于乔姆斯基的理论，但近来的突破却是基于统计的方法。乔姆斯基认为统计的方法不“优雅”，只是模仿而不是理解。会骑自行车不算理解，对自行车为什么不倒，能说三道四，才算理解。

7. 谷歌的研发总监诺维格为统计方法辩护说：简单的模型（如乔姆斯基理论，以及后来的各种改进版本）不能解决复杂的问题，人工智能的进一步发展必须两条腿走路。

8. 神经网络由一层一层的神经元构成。层数越多，就越深，所谓深度学习就是用很多层神经元构成的神经网络达到机器学习的功能。

9. 辛顿
    * 辛顿是深度学习的先驱，他和学生在2006年发表的两篇文章开辟了这个新领域，其中登在《科学》上的那篇提出了降维和逐层预训练的方法，使得深度网络的实用化成为可能。
    * 深度学习的实测效果很好，辛顿一直用深度信任网络做图像识别。在2012年举办的图像识别国际大赛中，辛顿团队的SuperVision以绝对领先的成绩击败众竞争对手巴得头筹。
    * 2009年，微软研究院的邓力小组开始和辛顿合作，用深度学习加上隐马尔可夫模型开发可实用的语音识别和同声翻译系统，2011年取得突破。

## 第6章 计算机下棋简史

1. 曼彻斯特大学的普林茨在1951年写了一个残局程序，能在离将死还有两步的情况下，找到最优解。这个问题也被成为“两步将死”（mate-in-two）问题。

2. 跳棋
    * 1951年斯特拉切在曼彻斯特Mark-1上写了第一款跳棋程序，但被图灵轻松击败。
    * 1956年IBM的塞缪尔写了第二个跳棋程序，其特点是自学习，这也是最早的机器学习程序之一，后来不断改进，曾经赢过盲人跳棋大师。
    * 20世纪80年代末，最强的跳棋程序一直就是加拿大阿尔伯塔大学的Chinook，作者是现任阿尔伯塔大学理学院院长的计算机系教授舍佛。
    * 2007年，舍佛团队证明对于跳棋，只要对弈双方不犯错，最终都是和棋，而Chinook已经可以不犯错。

3. 理论研究的开始
    * 香农1950年在《哲学杂志》发表“计算机下棋程序”一文，开启了计算机下棋的理论研究，其中主要思路在“深蓝”和AlphaGo中还能看到。
    * 香农把棋盘定义为二维数组，每个棋子都有一个对应的子程序计算棋子所有可能的走法，最后有个评估函数。

4. Minimax算法和a-b剪枝
    * 冯诺依曼和经济学家摩根斯顿合作的《博弈论》1944年出版，其中首先提出两人对弈的Minimax算法。
    * Minimax算法中，对弈双方分别为max和min，其对弈形成了博弈树，书的增长是指数式的，当树很深时，树的规模会变得不可控。
    * 麦卡锡首先提出a-b剪枝术以控制树的增长，而纽厄尔、司马贺和肖首先编程实现了a-b剪枝技术。
    * 平均而言，在同样资源限制下，a-b剪枝术要比原始Minimax算法搜索的树深度多一倍，也就是说可以比Minimax向前看的步数多一倍。

5. 1997年5月11日，在人机国际象棋对决中，卡斯帕罗夫认输，“深蓝”成为第一位战胜当时世界冠军的机器。

6. 蒙特卡洛方法
    * 蒙特卡洛方法最常用的教学例子就是计算圆的面积：在一个正方形里贴边画一个圆，然后随机向这个正方形里扔沙粒，扔到足够多时，开始数有多少沙粒落在圆里，结果除以所扔沙粒总数再乘以正方形面积，就是圆的面积。
    * 计算机下围棋也可以借鉴求圆面积的思路，随机模拟对弈双方走棋，当走棋的次数很多时，就可算出下棋点的概率，然后挑概率最大的地方落子。

7. 强化学习
    * 谷歌的AlphaGo首次引用了强化学习，使得机器和它自己对弈学习。
    * 强化学习从20世纪80年代就被发明，但已知不被重视，是AlphaGo使得它发出亮光。

## 第7章 自然语言处理

1. 乔治敦实验
    * 1953年至1954年，IBM资助美国乔治敦大学进行了有史以来的第一次机器翻译。
    * 1964年，美国政府的科研资助机构意识到机器翻译的研究进展缓慢，于是责成美国科学院对现状做一总结。
    * 调研发现，机器翻译比人翻译更慢，更不准确，而且成本更高（估算比人贵两倍）。结论是机器翻译在可预见的未来没法应用，应该立即停止对机器翻译的资助，转而支持一些更基础的、辅助性的研究，如电子词典等。

2. 乔姆斯基
    * 乔姆斯基之于语言学和认知科学，就像图灵之于计算机科学。
    * 乔姆斯基的句法频谱后来被证明和几种自动机有着深刻的关联：乔姆斯基3型文法（正则表达式）等价于有限自动机，2型文法（上下文无关文法）等价于下压自动机，1型文法（上下文相关文法）等价于线性有界非确定图灵机，0型文法等价于图灵机。
    * 按照乔姆斯基句法分析，句子可以通过一系列规则得到解析。一个句子可以解析成名词词组（NP）和动词词组（VP），而名词词组和动词词组又可再被解析。
    * 乔姆斯基认为，所有的语言（人工或者自然）都有与此类似的句法结构，并进一步指出语言的结构是内在的，而不是通过经验习得的。

3. 聊天机器人
    * 魏森鲍姆最显赫的成就是发明了对话程序ELIZA，现在对话程序有一个更流行且形象的词儿“聊天机器人”，但根儿都在ELIZA。
    * 科尔比感兴趣的问题正好和魏森鲍姆相反：怎样构造一个能聊天的病人，一方面可以培训心理医生，另一方面理解病人的征兆。他的成果在1972年变成了计算机程序PARRY。
    * 从ELIZA和PARRY分别的表现来看，现在的小冰等聊天机器人也没进步很多，但知识库的增大使得现在的聊天机器人更加实用。
    * 2014年，《纽约时报》记者纽曼撰文讲述她患自闭症的儿子嘎斯（Gus）在和苹果Siri的聊天过程中增强了和现实社会打交道的经验。

4. 维诺格拉德和积木世界
    * 维诺格拉德准备博士论文题目时，本来想模拟儿童世界，后来发现儿童的知识还是太深，需要有更简单的语言世界。最终他发明了“积木世界”，并将其命名为SHRDLU。
    * 人可以通过简单的自然语言，命令一个机器手对这个积木世界进行虚拟操作，例如拿起一个特定的积木块把它摞在另一个积木块上。当机器吃不准人的命令时，可以向人发问。
    * SHRDLU要远比ELIZA复杂，学术意义也更加深刻。SHRDLU把当时很多AI技术整合在一起，除了自然语言理解外，还有规划和知识表示。这甚至是最早的计算机图形学的应用。
    * 积木世界涉及了语言的好几个方面：语言的输入输出和生成，知识表示和理解，世界和思想。
    * 维特根斯坦说意义就是语言的使用。积木世界就是语言游戏，是研究语言的一种方法。语言的使用就是心和物之间的交互。

5. 统计派的复兴
    * 在1988年的计算语言学会议上，IBM TJ Watson研究中心机器翻译小组发表了统计机器翻译的论文，并推出法语/英语的翻译系统CANDIDE，这标志着统计派在大数据的支持下又回来了。
    * 贾里尼克：“我每开除一名语言学家，我的语音识别系统的性能就提高一点。”
    * 欧赫的博士论文是用大量平行语料构建语言模型和翻译模型，他加入谷歌后，谷歌海量的数据让欧赫如鱼得水，而谷歌翻译器迅速成为行业标杆。
    * 统计方法的另一个好处是工程师根本不需要语言学知识，也不需要懂源语言或目标语言，就可从事机器翻译。谷歌翻译团队就没什么科班出身的语言学家。
    * 乔姆斯基排斥统计方法的理由很简单，语言的可能性是无限的，统计不可能解决问题。

6. 神经翻译
    * 2016年，谷歌发布神经机器翻译系统GNMT，再次大幅提高机器翻译的水平。和谷歌更早期的基于短语的机器翻译不同，神经翻译的基本单位是句子，而误差降低了60%.
    * 谷歌使用了循环神经网络RNN做序列到序列的学习，硬件设备是谷歌自己的TensorFlow平台。
    * 2017年，Facebook进一步提升了翻译效率。他们用自己擅长的卷积神经网络CNN，进行序列到序列的学习。

7. 问答系统
    * 问答系统有三个必备的组成部分，第一部分是问题理解，第二部分是知识查询，第三部分是答案生成。第一部分和第三部分是自然语言处理的工作，它们通过知识图谱被有机地整合在一起。
    * 正是在问答系统的研究中发现了定理证明方法在知识表示上的局限。现在的问答系统依赖常识和知识，同时也依靠浅层的推理，知识图谱是核心。
    * 当知识图谱足够大的时候，它回答问题的能力会惊人。2011年IBM的沃森在美国电视智力竞赛节目Jeopardy!中击败人类选手，并获得百万美元大奖。沃森的知识图谱包括WordNet，Dbpedia和Yago。
    * 沃森还使用了开源搜索引擎。把搜索的结果文档的标题与维基百科词条进行匹配，如果在维基百科中能找到，就把搜索结果列入候选答案，在把候选答案反馈给搜索引擎，进一步对返回结果做证据支持的处理，然后给出答案。
    * 沃森的硬件系统是一个有90台IBM Power 750的集群，每台配一个IBM Power 8核处理器，每核4线程，所以一共720核，2880线程；内存16TB，所有的知识图谱都放在内存里了。

## 第8章 向自然学习：从遗传算法到强化学习

1. 人工智能从生物学里找计算模型的两条脉络
    * 麦卡洛克和皮茨的神经网络，演化到今天成了深度学习
    * 冯诺依曼的细胞自动机，历经遗传算法、遗传编程，其中一条支线最终演变成今天的强化学习。

2. 遗传算法
    * 霍兰德在回顾自己的研究生涯时说，如果一个人在早期过深地进入一个领域，可能会不利于吸收新的思想。
    * 对霍兰德影响最大的一本书是英国统计学家费舍的《自然选择的遗传理论》。费舍把孟德尔的遗传理论和达尔文的自然选择结合起来。霍兰德由此得到启发：进化和遗传是族群学习的过程，机器学习可以此为模型。
    * 遗传算法就是模拟种群的进化过程。

3. 遗传编程
    * 物理学家多依奇用生物进化来类比知识的进化。他说猜想就像变异，批评和实验就像选择，而交叉学科就是配对了。
    * 遗传编程：一组程序就一个特定的问题给出解答，按照执行结果的好坏给所有程序排序。程序本身也是数据，自然也可以修改。在遗传编程里，变异就是对程序做微小调整。交叉和配对就是将两个表现优异的程序互相嫁接。

4. 强化学习
    * 萨顿一直对研究动物怎么适应环境感兴趣，比如一个刚出生的孩子，怎么学会对环境的适应。在监督式学习中，目标是清楚的。但婴儿不知道目标是什么，不知道自己要什么。通过与外部世界的不断交互，婴儿受到奖励或惩罚，由此强化对外部世界的认知。
    * 强化学习的理论基础之一是马尔可夫决策过程。强化学习的主体是Agent，Agent和环境互动。强化学习就是Agent根据经验改变策略以期达到长期最大奖赏的过程。
    * 强化学习的另一个理论基础是动态规划。
    * 巴托和萨顿有时也把强化学习称为“享乐主义”，也即学习系统想最大化环境对自己的某种反馈。
    * 强化学习中有所谓“抬头看路”（探索）和“低头拉车”（苦干）之分。探索就是看看有没有别的选择，苦干就是专注于当前的选择。在强化学习中，学习率的值越小，能用于探索的时间就越少，绝大部分时间是在苦干。这就像我们的人生，大部分时间在被压榨，极少时间可以“诗和远方”。
    * 一旦一个算法被天才发明，并成功地在一个领域里得到应用，自然会有二流人才前赴后继把这个算法在其他领域发扬光大。20世纪80年代的神经网络如此，当下的强化学习也如此。

## 第9章 哲学家和人工智能

1. 得雷弗斯从以下四个层面批评人工智能
不同层面 | 人工智能假设 | 得雷弗斯的反驳
-------- | ------------ | -------------
生物层面 | 麦卡洛克-皮茨的神经元是二元的，像布尔电路 | 人脑是模拟的
心理学层面 | 纽厄尔-司马贺的信息处理和规则 | 常识和背景无法用规则表示
认识论层面 | 麦卡锡：所有知识都可以形式化  | 人的知识不是形式化的。可以用微分方程描述星体运动，不意味着星体在求解微分方程
本体论层面 | 世界由事实构成，方法论是还原论 | 人是人，物是物。物理的东西是还原论。人需要现象学。

2. 哲学家的分类
    * 哲学家有两类，一类是深刻的，一类是混饭的。罗素和弗里格是深刻的，没有他们，就不会有数理逻辑，也就不会有哥德尔、丘奇、图灵，以及后来的计算机科学。但没有现代的欧陆哲学，世界不过省了些粮食而已。
    * 没有胡塞尔和海德格尔，明斯基照样会想出“框架”，从而催生后来的“面向目标的程序设计”方法论。所谓“顶层”概念就是Java程序设计语言里的Object，或者知识图谱DBpedia里的Thing。

3. 塞尔和中文屋
    * “中文屋”思想实验：假设有个只懂英文不懂中文的人（塞尔的第一人称“我”）被锁在一个房间里，屋里只给“我”留了一本手册或一个计算机程序，这个手册或程序教“我”在收到中文信息时如何用中文应对。屋外的人用中文问问题，屋里的“我”依靠程序用中文回答问题，沟通方式是递纸条。塞尔的问题是：假设屋外的人不能区分屋里的人是不是母语为中文，那么屋里的“我”是不是就算懂中文？塞尔自己认为“我”不懂中文。
    * 塞尔的论断是屋里人即使查遍手册，顶多算是理解语法，而不算理解语义。我们可以问塞尔这样的问题：一个戴眼镜的人能算看见东西吗？一个耳聋的人通过换上人工耳蜗重获听觉后算是能听见吗？一个坐飞机的人算能飞吗？如果对这些问题的答案都是“算”，那中文屋作为一个系统为什么不算理解中文呢？
    * 塞尔的第一个准备好的答辩就是所谓“机器人”反驳。如果那本手册或者那个程序那么厉害，如果把它放在一个机器人里，那么这个机器人就可以做很多人可以做的事情，那么它是不是就算能理解了呢？塞尔的答辩恰恰说明单纯的形式化符号操纵是没有理解力的。

4. 普特南和缸中脑
    * 普特南虽是哲学出身，但他也是解决希尔伯特第十问题的主要推手之一，他和逻辑学家戴维斯长期合作研究机器定理证明，是这个领域的开拓者之一。
    * 1981年普特南出版了《理性、真理与历史》一书，该书的开篇就给出了“缸中脑”的假想实验。

## 第10章 人是机器吗？——人工智能的计算理论基础

1. 人是机器吗？
    * 认为人是机器：人也是由各种物理化学机制构成的，当然是机器了。
    * 认为人不是机器：人有很多功能，目前机器无法完成，尤其是那个叫“灵魂”的神奇东西。
    * 如果我们把“智能”当做人类特有的性质，那么“人是机器吗”这个问题就等价于“机器有智能吗”。

2. 丘奇-图灵论题
    * 丘奇-图灵论题：所有功能足够强的计算装置的计算能力都等价于图灵机。
    * 这是一个观察，而不是定理。图灵、丘奇、克里尼等人证明了当时所有数学家和逻辑学家想出的各种计算装置（例如递归函数、lambda演算、Post系统、图灵机等）都可以互相模拟。
    * 很容易证明乔姆斯基0型语法等价于Post系统，那自然也等价于图灵机了。
    * 天才数学家冯诺依曼发明的细胞自动机后来被天才物理学家沃尔弗拉姆发扬光大，也被证明和图灵机等价。
    * 图灵在发明图灵机时，还定义了Universal Turing Machine，简称UTM，译为“万能图灵机”或“通用图灵机”。UTM的核心思想就是一个图灵机的执行过程也可被编码成数据，放在纸带上，这样一个图灵机就可以把被编码的图灵机指令读出来，一步一步地执行，从而模仿这个特定图灵机的行为。这样，这台能模仿其他图灵机的图灵机就成了万能图灵机。这是一个很深刻的思想，现在的软件产业都得益于此：被编码的图灵机就是软件。
    * 后来冯诺依曼设计的计算机被人称为冯诺依曼架构，其最核心的思想就是存储程序，这个思想其实就是来自万能图灵机：被编码的图灵机就是存储的程序。冯诺依曼的架构的真正原创是随机寻址。

3. 相似性原则
    * 理论计算机科学家洪加威在20世纪80年代提出了相似性原则：计算装置之间互相模拟的成本是多项式的，也就是说靠谱的计算装置之间并不存在原则上的差异。相似性原则，类似于丘奇-图灵论题，是观察而不是数学定理。

4. 20世纪80年代初就有人证明三层以上的神经网络可以逼近任意连续函数。80年代末期，Steve Judd在他的博士论文里证明了三层以上的神经网络学习问题在图灵机上是NP完全的。

5. 近来也有自底向上的思路，例如细胞自动机，利用很简单的几条规则，就可展示很复杂的行为。沃尔夫勒姆在他的《新科学》一书中提到的这种现象其实早就被数学家康韦（John Conway）观察到，他设计了《生命游戏》（Game of Life），企图利用细胞自动机来说明确定性和自由意志的问题。高德纳在评价康韦的工作时说：所有规则都是确定性的，但游戏的演进过程却给人一种自主性的感觉。

## 第11章 智能的进化

1. 为什么人比其他动物聪明
    * 人脑中总共有860亿个神经元，其中大脑皮层有160亿个神经元；大象的脑子总共有2570个神经元，其中98%的神经元都存在于大象的小脑中，其大脑皮层只有56亿个神经元。大脑皮层的神经元数量决定了动物的智力水平，人的大脑皮层中神经元数量远高于其他物种，所以人类比其他物种更聪明。
    * 大脑皮层中的神经元数量越大，消耗也越大。人脑每天消耗的能量占人体全部耗能的25%.人之所以能够很快超越其他物种，主要是因为人类掌握了烹饪技术。能够在短时间内摄入大量卡路里以支持大脑运转。其他物种则将摄入的卡路里用于维持身体运转，不得不牺牲大脑皮层的神经元数量。直立行走的动物要比四足动物更省能耗。

2. 《西方将主宰多久》的观点
    * 西方除了在中世纪（公元500年到公元1500年）的大约1000年期间外，一直都是领先东方的。
    * 东方真正发达起来是在19世纪中叶之后，西方教会了东方新的能量摄取手段。

3. 曾经有人说，人对机器的最简单控制就是断电，但现在最简单的扫地机器人也知道快没电时找回基座充电。

4. 把人工智能教科书中的n种智能功能整合起来（例如下棋、图像识别、语音识别、规划等），是不是就会达到强人工智能或者超智能？如果这个整合的整体在我们可以想到的各个方面都超过人类，那我们如何对付这个新时代的“弗兰肯斯坦”？

5. 图灵在1950年那篇被广为引用的文章《计算机与智能》的结尾处说：“我们只能看到当下，但看见的这些就够我们忙活的了。”
> "We can only see a short distance ahead, but we can see plenty there that needs to be done." (Turing)

## 附录1 图灵小传

1. 图灵生于1912年6月23日，逝于1954年6月7日，活了不到42岁。

2. 他三岁时，他妈到伦敦看完他又要回印度，临别时对他说：“当个乖孩子，啊！”图灵回道：“但有时我会忘的。”

3. 图灵上数学课不听讲，也不看书，所有定理都是自己推出来的。如果自己推对了，考试成绩就好，自己推错了，成绩就不好。所有中学以前的数学知识他自己从头发明了一遍。化学课也如此，他自己发明了从海藻里分离碘。

4. 图灵父母为避税，定居法国。图灵兄弟俩只能从法国过英吉利海峡去各自的学校。图灵在南安普顿上岸太晚，结果所有去学校的交通都没了，于是他从行李里拿出自己的自行车，买了张地图，就向学校骑行。车太不给力，中途坏了两次。60英里地，他走了一夜，中间还住了五星级酒店。最后图灵把五星级酒店的发票给父母寄去，表示自己没乱花钱。

5. 图灵对逻辑感兴趣大约是在1933年，那时他读到罗素的《数理哲学导论》。1939年图灵回剑桥大学教《数学基础》课，而同一学期维特根斯坦也在开一门同名的课程。图灵是讲数理逻辑，而老维则在讲数学哲学。图灵出于好奇，去旁听维特根斯坦的《数学基础》课。整个课程变成了老维和小图的对话。

6. 图灵自己回忆他是躺在草坪上把图灵机的构造想明白的，他看到哥德尔那篇文章后就开始琢磨图灵机。图灵在剑桥的导师纽曼看到丘奇发表的lambda演算的文章，就把图灵推荐给丘奇，让小图跟丘奇读博士，并告诉丘奇：这孩子搞了个图灵机。

7. 图灵在与德国Enigma加密机的斗争中，逐渐形成了如何建造一台实用的通用计算机的思路。1946年年初，他向国家物理实验室提交了ACE的报告，这份报告比冯诺依曼的EDVAC报告晚了几个月。所以大家还是觉得老冯是最早的计算机设计师，但老冯逢人就说“这都是图灵的主意”。

8. 图灵的长跑记录是奥运水平的。在布莱彻利庄园工作时，经常要到伦敦开会，战时单位派车不方便，图灵也不摆谱，说一句“我自己解决吧”，64公里路，跑着去，完事再跑着回来。

9. 曼彻斯特公园里图灵雕像的底座上引用了罗素的话：“数学不仅有真理，也有最高的美，那是一种冷艳和简朴的美，就像雕塑。”


## 附录2 人工智能前史：图灵与人工智能

1. 图灵1948年的文章提到了“肉体智能”和“无肉体智能”的区分，他明确列出五个领域属于无肉体智能：（1）博弈（如下棋），（2）语言学习，（3）语言翻译，（4）加密学，（5）数学（所谓数学就是定理证明）。

2. 图灵在撰写1950年的文章时不仅提出了问题（“机器能思维么？”），还提出了问题的各种变种；不仅给出了答案，还预想了答案的可能异议，以及对异议的反驳。

3. 图灵1936年发表“可计算书”一文。

4. 冯诺依曼多次向同事和部下指出Stored-Program（所谓冯诺依曼架构的核心）就是通用图灵机（UTM）的原创概念，应该全部归功于图灵。冯诺依曼架构中真正原创的是随机存取内存。从这个角度看，图灵机是现代计算机的基础。

## 附录3 冯诺依曼与人工智能

1. 计算机科学一直都有两条互相交错的路线，工程路线终究可以追溯到冯诺依曼，而理论的起源则在图灵。

2. 冯诺依曼发现了哥德尔定理的重要性，他称哥德尔是亚里士多德以来最伟大的逻辑学家。

3. 冯诺依曼在计算机工程的开创新工作是计算机产业的基础。他牵头撰写的EDVAC报告定义了“冯诺依曼”架构，后来IBM等所有其他计算机项目都以此为基础建造计算机。EDVAC报告中最核心的概念是“存储程序”（Stored Program），冯诺依曼把这个概念的原创权公正无私地给予了图灵。

4. 冯诺依曼1948年在加州理工学院所在地帕萨迪纳召开的Hixon会议上的研究“自动机的通用和逻辑理论”被收入他的全集，这篇文章开启了细胞自动机的理论研究。冯诺依曼在细胞自动机和DNA的工作间接影响到了霍兰德，也影响了天才沃尔弗拉姆。

5. 冯诺依曼和经济学家摩根斯顿合作的《博弈论》影响了另一位数学天才纳什。纳什证明了两人零和游戏中存在Minimax策略。从某种意义上，这是计算机经典算法alpha-beta的前兆。

6. 《计算机与大脑》的第一部分是“计算机”，第二部分是“大脑”，但冯诺依曼没有把这两条路线对立，他认为这是解决同一问题的两种方法。

