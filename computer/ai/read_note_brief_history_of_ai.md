# 《人工智能简史》读书笔记

## 前言

1. 作者赞赏的科普书
    * 安德鲁·霍奇斯的《艾伦·图灵传：如谜的解谜者》是内行写作的典范。
    * 安妮塔·费佛曼的两本逻辑学家传记是我心目中的标杆。

2. 写作目的
> 本书除了想梳理始于20世纪40年代的人工智能的历史外，还有一个作者隐含的心愿：作为人工智能的科普。

## 第1章 达特茅斯会议：人工智能的缘起

1. 人工智能的前戏
    * 1955年美国西部计算机联合大会中还套了一个小会：学习机讨论会。
    * 讨论会中塞弗里奇发表了一篇模式识别的文章，纽厄尔则探讨了计算机下棋，他们分别代表两派观点。
    * 神经网络的鼻祖之一皮茨的总结：“（一派人）企图模拟神经系统，而纽厄尔则企图模拟心智......但殊途同归。”
    * 这预示了人工智能随后几十年关于“结构与功能”两个阶级、两条路线的斗争。

2. 人物渊源
    * BASIC语言发明人（克门尼）曾是LISP语言发明人（麦卡锡）的老板。因为克门尼刚上任达特茅斯学院的系主任后，就从母校普林斯顿大学带回了包括麦卡锡在内的四位刚毕业的博士。
    * 麦卡锡的老师是失去双手的代数拓扑学家莱夫谢茨。
    * 明斯基的老师塔克是莱夫谢茨的学生。
    * 塞弗里奇在麻省理工学院时和麦卡洛克（神经网络的开创人之一）一起在维纳手下工作。
    * 香农当时是贝尔实验室的大佬。
    * 纽厄尔是麦卡锡和明斯基的同龄人，而司马贺比他们三人都大11岁。司马贺是纽厄尔的老师。

3. 纽厄尔和司马贺代表了人工智能的“符号派”，他们提出“物理符号系统假说”，简言之：智能是对符号的操作，最原始的符号对应于物理客体。

4. 麦卡锡和明斯基的建议书里罗列了他们计划研究的7个领域：
    * 自动计算机（“自动”指的是可编程）
    * 编程语言
    * 神经网络
    * 计算规模的理论，即计算复杂性
    * 自我改进，即机器学习
    * 抽象
    * 随机性和创见性

5. 会议中给所有人留下最深印象的是纽厄尔和司马贺的一款程序“逻辑理论家”
    * 这款程序可以证明怀特海和罗素《数学原理》中命题逻辑部分的一个很大子集。
    * 麦卡锡回忆说他从纽厄尔和司马贺的IPL语言中学到了表处理，这成为他后来发明LISP的基础。

6. 戴森在他的《一面多彩的镜子》一书中借鉴过柏林“刺猬与狐狸”的比喻
    * 刺猬是那些构建理论体系的人，狐狸则是那些解决问题的人
    * 在戴森眼里，爱因斯坦、哥德尔是刺猬，而费米、冯·诺依曼属于狐狸。

7. 乔姆斯基晚年和物理学家克劳斯对话时被问及“机器可以思维吗？”，他套用计算机科学家Dijkstra的说法反问：“潜艇会游泳吗？”

## 第2章 自动定理证明兴衰纪

1. 数学哲学有三大派：
    * 逻辑主义：代表人物是罗素，主旨是把数学归约到逻辑，这样只要把逻辑问题解决了，之上的数学问题自然就解决了。
    * 形式主义：代表人物是希尔伯特，他的梦想是把数学形式化，数学过程就是把一串符号变成另一串符号。他设想如果能设计一个大一统的算法，那么所有的数学问题都可以由这个算法来解答。哥德尔后来证明这一切是不可能的。
    * 直觉主义：代表人物是布劳威尔。

2. 自动定理证明起源于逻辑，初衷就是把逻辑演算自动化。逻辑学的源头是亚里士多德的三段论。

3. 戴维斯和普特南
    * 戴维斯于1954年完成了第一个定理证明程序，实现了普利斯博格算术的判定过程，可以证明“两个偶数之和还是偶数”之类的问题。
    * 戴维斯最重要的贡献是和哲学家普特南等人解决了希尔伯特第十问题（不定方程的可解答性）。
    * 普特南提出的“缸中脑”是最常被引用的假想实验之一。
    * 戴维斯和普特南在机器定理证明上合作的成果是影响广泛的戴维斯-普特南（Davis-Putnam，简称DP）过程，以及后来的DPLL。

4. 王浩
    * 1958～1959年实现了一个完全的命题逻辑程序和一个一阶逻辑程序，可以证明《数学原理》全部150条一阶逻辑以及200条命题逻辑定理。
    * 王浩研究了AE形式（即前面是全程量词，后面是存在量词）和AEA形式的可计算性和复杂性，由此引出了他的学生库克的NP理论。
    * 库克1971年发表的文章的题目是《定理证明的复杂性》，因此获得1982年图灵奖。
    * 王浩的定理证明研究孕育了整个理论计算机科学，他的定理证明程序后来也成为高级语言的基准程序。
    
5. 罗宾逊
    * 阿兰·罗宾逊拓展了普拉格维茨的原始合一算法，发明了归结原理。
    * 以前的定理证明技术会用到很多规则，现在所有证明推导只要有归结这一条规则就可以了。
    * 罗宾逊的贡献在于一系列工作的综合，除了归结外，还有合一和包含。

6. 马库恩
    * 用C语言写了Otter定理证明器，Otter实现了当时定理证明里最先进的所有技术。
    * 最早把项索引引入到机器定理证明器，并发明了差别树索引，极大地提高了证明的效率。
    * 利用Otter的模块开发了另一款专门证明方程的证明器EQP，并在1996年10月10日用EQP证明了罗宾斯猜想。

7. 哥德尔证明一阶整数（算术）是不可判定的，但几乎在同时塔尔斯基则证明一阶实数（初等几何和代数）是可判定的。

8. 数学家吴文俊在研究中国数学史时，受到塔尔斯基算法的启发，针对某一大类的初等几何问题给出了高效的算法，后来还推广到一类微分几何问题上。

9. 波尔和摩尔
    * 形式主义定理器中最广为人知的当属波尔-摩尔证明器及其一系列后续变种。波尔-摩尔证明器的核心是数学归纳法和项重写，可用来证明软件和硬件的正确性。
    * 波尔-摩尔字符串匹配算法（Boyer-Moore string search algorithm）是目前最快的字符串匹配算法，其起因也是定理证明。
    * 他们最早用来实现定理证明器的语言是InterLisp，但后来迅速发现InterLisp内带的朴素字符串搜索太慢，于是发明了自己的字符串匹配算法。

10. 定理证明的过程，都是一个归约的过程，无论是逻辑派的（即把数学问题归约到更基本的逻辑问题），还是形式派的（即用一套规则不断地变换给定的公式直到显性的形式出现）。

11. 定理证明的应用
    * 定理证明是极端的符号派。
    * 所有符号派的人工智能技术的基础都是定理证明，如专家系统、知识表示和知识库（甚至数据库）。
    * 专家系统的很多术语都是重新包装过的定理证明术语。如“知识库”就是“公理集合”，“规则库”就是“支持集”，“推理引擎”更是直接照搬。
    * 当下流行的知识图谱的基础也是定理证明技术——知识表示的理论“描述逻辑”就是被约束的一阶逻辑的子集。

12. 所有定理证明系统的一个致命问题是它们多是独立的，很少和其他数学工具结合，结果必然是只能是玩具系统，而不具实用性。

## 第3章 从专家系统到知识图谱

1. 费根鲍姆、李德伯格和翟若适合作发明了第一个专家系统DENDRAL。DENDRAL输入的是质谱仪的数据，输出是给定物质的化学结构。

2. 肖特莱福在布坎南的指导下发明了专家系统MYCIN，一个针对细菌感染的诊断系统。MYCIN的处方准确率为69%，当时专科医生的准确率为80%，MYCIN的成绩已经优于非本专业的医生。

3. MYCIN首创了后来作为专家系统要素的产生式规则：不精确推理。DENDRAL的初衷则是从专家采集来的数据做机器归纳，或者说机器学习。

4. 专家系统最成功的案例是DEC的专家配置系统XCON。当客户订购DEC的VAX系列计算机时，XCON可以按照需求自动配置零部件。从1980年投入使用到1986年，XCON一共处理了八万个订单。

5. 心理学家米勒和乔姆斯基等一起开拓了认知科学，晚年还带领普林斯顿大学的认知科学实验室同仁做了WordNet（“词网”）。WordNet不单是一个同义词辞典，还定义了词的上下位关系，WordNet成为自然语言处理的基本工具。

6. 雷纳特
    * 当雷纳特来到MCC（微电子与计算机技术公司）时，他已经有了一个新想法：把人类的常识编码，建成知识库。这个新项目叫Cyc。
    * Cyc取自英文单词encyclopedia（百科全书），这其实就是最早的知识图谱。
    * 雷纳特坚定地支持他老师费根鲍姆的知识原则：一个系统之所以能展示高级的智能理解和行为，主要是因为在所从事的领域所表现出来的特定知识：概念、事实、表示、方法、比喻以及启发。雷纳特甚至说：“智能就是一千万条规则。”
    * 雷纳特曾说：“学习只在已知事物的边缘发生，所以人们只可能学到与自己已知相似的新东西。如果你试图学习的东西与你已知的东西距离不远，那么你就能学会。这个边缘的范围越大（你已知的东西越多），就越有可能发现新的东西。”

7. 归根到底，专家系统的理论基础依然是机器定理证明。

8. 知识与推理
    * 如果从纯粹的定理证明的角度简单地看专家系统，所谓知识其实就是公理，公理越多，推理的步骤自然就会越少。
    * 所谓知识和推理的对立，其实是狭义（特殊目的）和广义（通用）的区别。知识是狭义的，推理是广义的，因为不需要过多的公理。
    * 狭义对机器的短期实现高效，但人的学习门槛较高；广义对机器的学习自然低效，但人学习的门槛较低。
    * 一阶逻辑的学习门槛最低，但当知识库变大，推理引擎也得变得更加专用才能高效。

## 第5章 神经网络简史

1. 1943年，麦卡洛克和皮茨发表了第一篇模拟神经网络的原创文章。

2. 罗森布拉特和感知机
    * 1957年，康奈尔大学的实验心理学家罗森布拉特模拟实现了一种叫做“感知机”的神经网络模型。
    * 罗森布拉特在理论上证明了单层神经网络在处理线性可分的模式识别问题时可以收敛，并以此为基础做了若干“感知机”有学习能力的实验。
    * 明斯基和佩珀特在《感知机：计算几何学》一书中证明单层神经网络不能解决XOR（异或）问题，进而证明神经网络的计算能力实在有限。
    * 明斯基呈现出的“感知机”的缺陷，对罗森布拉特是个致命打击，原来的政府资助机构也逐渐停止对神经网络研究的支持。

3. 1974年，哈佛大学的沃波斯的一篇博士论文证明了在神经网络多加一层，并且利用“后向传播”学习方法，可以解决XOR问题。但由于当时是神经网络研究的低估，文章不受重视。

4. 霍普菲尔德
    * 神经网络在20世纪80年代的复兴归功于物理学家霍普菲尔德。
    * 1982年，加州理工学院的生物物理教授霍普菲尔德提出一种新的神经网络——“霍普菲尔德网络”，可以解决一大类模式识别问题，还可以给出一类组合优化问题的近似解。

5. 连接主义运动
    * 一帮早期神经网络研究的幸存者，在生物学家克里克和认知科学大佬诺曼的鼓励下，以加州大学圣地亚哥分校为基地，开始了连接主义运动，这个运动的领导者是两位心理学家鲁美尔哈特和麦克利兰德，外加一位计算机科学家辛顿。
    * 连接主义运动的成果之一是那本被称为PDP（Parallel and Distributed Processing）的著名文集，这本书被后来的神经网络新秀称为“圣经”。

6. 机器翻译的早期实践都源于乔姆斯基的理论，但近来的突破却是基于统计的方法。乔姆斯基认为统计的方法不“优雅”，只是模仿而不是理解。会骑自行车不算理解，对自行车为什么不倒，能说三道四，才算理解。

7. 谷歌的研发总监诺维格为统计方法辩护说：简单的模型（如乔姆斯基理论，以及后来的各种改进版本）不能解决复杂的问题，人工智能的进一步发展必须两条腿走路。

8. 神经网络由一层一层的神经元构成。层数越多，就越深，所谓深度学习就是用很多层神经元构成的神经网络达到机器学习的功能。

9. 辛顿
    * 辛顿是深度学习的先驱，他和学生在2006年发表的两篇文章开辟了这个新领域，其中登在《科学》上的那篇提出了降维和逐层预训练的方法，使得深度网络的实用化成为可能。
    * 深度学习的实测效果很好，辛顿一直用深度信任网络做图像识别。在2012年举办的图像识别国际大赛中，辛顿团队的SuperVision以绝对领先的成绩击败众竞争对手巴得头筹。
    * 2009年，微软研究院的邓力小组开始和辛顿合作，用深度学习加上隐马尔可夫模型开发可实用的语音识别和同声翻译系统，2011年取得突破。

## 第6章 计算机下棋简史

1. 曼彻斯特大学的普林茨在1951年写了一个残局程序，能在离将死还有两步的情况下，找到最优解。这个问题也被成为“两步将死”（mate-in-two）问题。

2. 跳棋
    * 1951年斯特拉切在曼彻斯特Mark-1上写了第一款跳棋程序，但被图灵轻松击败。
    * 1956年IBM的塞缪尔写了第二个跳棋程序，其特点是自学习，这也是最早的机器学习程序之一，后来不断改进，曾经赢过盲人跳棋大师。
    * 20世纪80年代末，最强的跳棋程序一直就是加拿大阿尔伯塔大学的Chinook，作者是现任阿尔伯塔大学理学院院长的计算机系教授舍佛。
    * 2007年，舍佛团队证明对于跳棋，只要对弈双方不犯错，最终都是和棋，而Chinook已经可以不犯错。

3. 理论研究的开始
    * 香农1950年在《哲学杂志》发表“计算机下棋程序”一文，开启了计算机下棋的理论研究，其中主要思路在“深蓝”和AlphaGo中还能看到。
    * 香农把棋盘定义为二维数组，每个棋子都有一个对应的子程序计算棋子所有可能的走法，最后有个评估函数。

4. Minimax算法和a-b剪枝
    * 冯诺依曼和经济学家摩根斯顿合作的《博弈论》1944年出版，其中首先提出两人对弈的Minimax算法。
    * Minimax算法中，对弈双方分别为max和min，其对弈形成了博弈树，书的增长是指数式的，当树很深时，树的规模会变得不可控。
    * 麦卡锡首先提出a-b剪枝术以控制树的增长，而纽厄尔、司马贺和肖首先编程实现了a-b剪枝技术。
    * 平均而言，在同样资源限制下，a-b剪枝术要比原始Minimax算法搜索的树深度多一倍，也就是说可以比Minimax向前看的步数多一倍。

5. 1997年5月11日，在人机国际象棋对决中，卡斯帕罗夫认输，“深蓝”成为第一位战胜当时世界冠军的机器。

6. 蒙特卡洛方法
    * 蒙特卡洛方法最常用的教学例子就是计算圆的面积：在一个正方形里贴边画一个圆，然后随机向这个正方形里扔沙粒，扔到足够多时，开始数有多少沙粒落在圆里，结果除以所扔沙粒总数再乘以正方形面积，就是圆的面积。
    * 计算机下围棋也可以借鉴求圆面积的思路，随机模拟对弈双方走棋，当走棋的次数很多时，就可算出下棋点的概率，然后挑概率最大的地方落子。

7. 强化学习
    * 谷歌的AlphaGo首次引用了强化学习，使得机器和它自己对弈学习。
    * 强化学习从20世纪80年代就被发明，但已知不被重视，是AlphaGo使得它发出亮光。

## 第7章 自然语言处理

1. 乔治敦实验
    * 1953年至1954年，IBM资助美国乔治敦大学进行了有史以来的第一次机器翻译。
    * 1964年，美国政府的科研资助机构意识到机器翻译的研究进展缓慢，于是责成美国科学院对现状做一总结。
    * 调研发现，机器翻译比人翻译更慢，更不准确，而且成本更高（估算比人贵两倍）。结论是机器翻译在可预见的未来没法应用，应该立即停止对机器翻译的资助，转而支持一些更基础的、辅助性的研究，如电子词典等。

2. 乔姆斯基
    * 乔姆斯基之于语言学和认知科学，就像图灵之于计算机科学。
    * 乔姆斯基的句法频谱后来被证明和几种自动机有着深刻的关联：乔姆斯基3型文法（正则表达式）等价于有限自动机，2型文法（上下文无关文法）等价于下压自动机，1型文法（上下文相关文法）等价于线性有界非确定图灵机，0型文法等价于图灵机。
    * 按照乔姆斯基句法分析，句子可以通过一系列规则得到解析。一个句子可以解析成名词词组（NP）和动词词组（VP），而名词词组和动词词组又可再被解析。
    * 乔姆斯基认为，所有的语言（人工或者自然）都有与此类似的句法结构，并进一步指出语言的结构是内在的，而不是通过经验习得的。

3. 聊天机器人
    * 魏森鲍姆最显赫的成就是发明了对话程序ELIZA，现在对话程序有一个更流行且形象的词儿“聊天机器人”，但根儿都在ELIZA。
    * 科尔比感兴趣的问题正好和魏森鲍姆相反：怎样构造一个能聊天的病人，一方面可以培训心理医生，另一方面理解病人的征兆。他的成果在1972年变成了计算机程序PARRY。
    * 从ELIZA和PARRY分别的表现来看，现在的小冰等聊天机器人也没进步很多，但知识库的增大使得现在的聊天机器人更加实用。
    * 2014年，《纽约时报》记者纽曼撰文讲述她患自闭症的儿子嘎斯（Gus）在和苹果Siri的聊天过程中增强了和现实社会打交道的经验。

4. 维诺格拉德和积木世界
    * 维诺格拉德准备博士论文题目时，本来想模拟儿童世界，后来发现儿童的知识还是太深，需要有更简单的语言世界。最终他发明了“积木世界”，并将其命名为SHRDLU。
    * 人可以通过简单的自然语言，命令一个机器手对这个积木世界进行虚拟操作，例如拿起一个特定的积木块把它摞在另一个积木块上。当机器吃不准人的命令时，可以向人发问。
    * SHRDLU要远比ELIZA复杂，学术意义也更加深刻。SHRDLU把当时很多AI技术整合在一起，除了自然语言理解外，还有规划和知识表示。这甚至是最早的计算机图形学的应用。
    * 积木世界涉及了语言的好几个方面：语言的输入输出和生成，知识表示和理解，世界和思想。
    * 维特根斯坦说意义就是语言的使用。积木世界就是语言游戏，是研究语言的一种方法。语言的使用就是心和物之间的交互。

5. 统计派的复兴
    * 在1988年的计算语言学会议上，IBM TJ Watson研究中心机器翻译小组发表了统计机器翻译的论文，并推出法语/英语的翻译系统CANDIDE，这标志着统计派在大数据的支持下又回来了。
    * 贾里尼克：“我每开除一名语言学家，我的语音识别系统的性能就提高一点。”
    * 欧赫的博士论文是用大量平行语料构建语言模型和翻译模型，他加入谷歌后，谷歌海量的数据让欧赫如鱼得水，而谷歌翻译器迅速成为行业标杆。
    * 统计方法的另一个好处是工程师根本不需要语言学知识，也不需要懂源语言或目标语言，就可从事机器翻译。谷歌翻译团队就没什么科班出身的语言学家。
    * 乔姆斯基排斥统计方法的理由很简单，语言的可能性是无限的，统计不可能解决问题。

6. 神经翻译
    * 2016年，谷歌发布神经机器翻译系统GNMT，再次大幅提高机器翻译的水平。和谷歌更早期的基于短语的机器翻译不同，神经翻译的基本单位是句子，而误差降低了60%.
    * 谷歌使用了循环神经网络RNN做序列到序列的学习，硬件设备是谷歌自己的TensorFlow平台。
    * 2017年，Facebook进一步提升了翻译效率。他们用自己擅长的卷积神经网络CNN，进行序列到序列的学习。

7. 问答系统
    * 问答系统有三个必备的组成部分，第一部分是问题理解，第二部分是知识查询，第三部分是答案生成。第一部分和第三部分是自然语言处理的工作，它们通过知识图谱被有机地整合在一起。
    * 正是在问答系统的研究中发现了定理证明方法在知识表示上的局限。现在的问答系统依赖常识和知识，同时也依靠浅层的推理，知识图谱是核心。
    * 当知识图谱足够大的时候，它回答问题的能力会惊人。2011年IBM的沃森在美国电视智力竞赛节目Jeopardy!中击败人类选手，并获得百万美元大奖。沃森的知识图谱包括WordNet，Dbpedia和Yago。
    * 沃森还使用了开源搜索引擎。把搜索的结果文档的标题与维基百科词条进行匹配，如果在维基百科中能找到，就把搜索结果列入候选答案，在把候选答案反馈给搜索引擎，进一步对返回结果做证据支持的处理，然后给出答案。
    * 沃森的硬件系统是一个有90台IBM Power 750的集群，每台配一个IBM Power 8核处理器，每核4线程，所以一共720核，2880线程；内存16TB，所有的知识图谱都放在内存里了。

